{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ec37788a-8092-4835-b93b-2a5ae537730d",
   "metadata": {},
   "source": [
    "---\n",
    "title: Data Preprocessing\n",
    "author: DSCI 571 - Supervised Learning I\n",
    "format: html\n",
    "keep-ipynb: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9a461-f076-48c2-a709-e3198afd4602",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cf04d-960d-4481-baef-0cfeeb733396",
   "metadata": {},
   "source": [
    "## Purpose: tackling missing values\n",
    " - Models are not able to deal with missing values (`NaN`s)\n",
    "   - See below for example\n",
    " - Possible solutions\n",
    "   - Delete the rows\n",
    "     - Cons: not good if dataset is small\n",
    "   - Imputation\n",
    "     - for Categories: fill by `“missing”` or `mode` of training data\n",
    "     - for Numerics: fill by `mean` or `median` of training data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846509d-a9a3-415e-a61c-2ba83973efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "# X_train.info() to visualize the problem\n",
    "\n",
    "\n",
    "# knn = kNeighborsRegressor()\n",
    "# knn.fit(X_train, y_train)\n",
    "# ValueError:...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fb7f5-91ac-4743-816f-6a5a4b912c3b",
   "metadata": {},
   "source": [
    "## How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4ca237-cae2-478e-9291-5b70febdd7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-130.0437</td>\n",
       "      <td>55.9773</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-134.4197</td>\n",
       "      <td>58.3019</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-123.0780</td>\n",
       "      <td>48.9854</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.7436</td>\n",
       "      <td>48.9881</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.2691</td>\n",
       "      <td>48.9951</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-72.7218</td>\n",
       "      <td>45.3990</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>-66.6458</td>\n",
       "      <td>45.9664</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-79.2506</td>\n",
       "      <td>42.9931</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-72.9406</td>\n",
       "      <td>45.6275</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-79.4608</td>\n",
       "      <td>46.3092</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     longitude  latitude country\n",
       "0    -130.0437   55.9773     USA\n",
       "1    -134.4197   58.3019     USA\n",
       "2    -123.0780   48.9854     USA\n",
       "3    -122.7436   48.9881     USA\n",
       "4    -122.2691   48.9951     USA\n",
       "..         ...       ...     ...\n",
       "204   -72.7218   45.3990  Canada\n",
       "205   -66.6458   45.9664  Canada\n",
       "206   -79.2506   42.9931  Canada\n",
       "207   -72.9406   45.6275  Canada\n",
       "208   -79.4608   46.3092  Canada\n",
       "\n",
       "[209 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-summary: prepare `df`\n",
    "#| code-fold: true\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/canada_usa_cities.csv\")\n",
    "df\n",
    "\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e73c8f-bb3a-4e4a-8ebc-7fba69afce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)\n",
    "\n",
    "X_train_imp = imputer.transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test) \n",
    "\n",
    "# FIXME: visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26680028-b466-435d-885e-6ecea21ee1bb",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c1844-7311-4b08-af40-00f471f522b8",
   "metadata": {},
   "source": [
    "## Purpose: for numeric features\n",
    " - Features with different scaling is a huge problem for $k$-NN and SVM\n",
    "   - Distance is dominated by the features with larger values\n",
    "   - Features with smaller values are being ignored, but they can be highly informative!\n",
    "   - FIXME: Example?\n",
    "   - Though not a problem for DecisionTree and Dummy\n",
    "     - DecisionTree looks at features one-by-one\n",
    "     - Dummy only looks at the target Y\n",
    " - Our models should not be sensitive to scales\n",
    " - Possible solutions\n",
    "   - Normalization: set range to [0, 1]\n",
    "     - `(value - min) / max`\n",
    "   - Standardization: standard the values s.t. sample (mean, sd) = (0, 1)\n",
    "     - `(value - sample_mean) / sd`\n",
    "   - FIXME: Other two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd2e2c-b744-4a38-b875-1fafd9661277",
   "metadata": {},
   "source": [
    "## How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275c013-b960-4ce8-9e5f-4b82672e2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6774f-4cec-472a-8193-8ffb3e30cc97",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc4dd3-0980-4916-8e51-10eb01e3cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# FIXME: (to visualize the result) pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b577150-cce3-4f79-958d-4b346b445aea",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e66c9c-1205-4e8d-8c77-7fdff791a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# FIXME: (to visualize the result) pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ed180-c7d3-4ca1-a76d-b6bd07928a03",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d489f7-f36b-45c3-a4c9-987802c53dfb",
   "metadata": {},
   "source": [
    "## Purpose: tackling categorical variables\n",
    "- In `scikit-learn`, most algorithms require numeric inputs\n",
    "  - e.g. for $k$-NN, unable to calculate distances\n",
    "  - `sklearn.DecisionTree` does not support categorical features\n",
    "    - although theoretically it should work\n",
    "    - see below for exmample (FIXME: ValueError: Cannot use median strategy with non-numerica data…)\n",
    "- Possible solutions\n",
    "  - Drop the column(s) (not recommended)\n",
    "    - those columns might be relevant to the target\n",
    "  - Transform categorical features to numerics. Two ways:\n",
    "    - Ordinal encoding\n",
    "    - One-hot encoding (OHE) $\\leftarrow$ recommended in most cases\n",
    "      - Create binary columns for each category in the feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c3cb55-f530-4dad-bc04-82a1dee5f395",
   "metadata": {},
   "source": [
    "## Ordinal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08305c-526f-4e2e-81ef-d15ee379f6e3",
   "metadata": {},
   "source": [
    "### How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0046067-c8b4-4e17-992a-e9a1f069560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cbcf55e-7439-45d4-aac2-cbda90735bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy (max_depth=1): 0.81\n",
      "Validation accuracy (max_depth=2): 0.81\n",
      "Validation accuracy (max_depth=3): 0.833\n",
      "Validation accuracy (max_depth=4): 0.833\n",
      "Validation accuracy (max_depth=5): 0.905\n",
      "Validation accuracy (max_depth=6): 0.881\n",
      "Test accuracy: 0.762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encode = OrdinalEncoder()\n",
    "encode.fit(X_train)\n",
    "X_train_ord = encode.transform(X_train) # use one feature X_train as example\n",
    "X_test_ord = encode.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75086b-e852-4c06-b5d4-558ca51bf939",
   "metadata": {},
   "source": [
    "We typically expect $E_{train} < E_{validation} < E_{test} < E_{deployment}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327ecce-af63-48c6-a72c-c77a6a457fd7",
   "metadata": {},
   "source": [
    "### Cons\n",
    " - Might have imposed unrealistic ordinality in the data\n",
    "   - i.e. not necessarily making sense on distancing\n",
    "   - In the example below, French and Hindi is closer than French and Spanish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7e53b-a83d-4a01-91de-021a6d5dce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train_ord, …)\n",
    "pd.concat([X_train, df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8716b-5c44-4c4d-aaf6-d253924b0a27",
   "metadata": {},
   "source": [
    "## One-hot encoding (OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80e91b-c139-4324-9ac4-e46e4873470c",
   "metadata": {},
   "source": [
    "### How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c590548-df8d-400f-b192-86ee11ff658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encode = OneHotEncoder(handle_unknown=”ignore”, sparse=False, dtype=”int”)\n",
    "encode.fit(X_train)\n",
    "\n",
    "X_train_ord = encode.transform(X_train) # use one feature X_train as example\n",
    "encode.categories_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342207e-6823-4d0b-8ebf-6e13e37e3221",
   "metadata": {},
   "source": [
    "# Combining ALL using **Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926aba7-63da-4332-ad5d-800bfd7f8590",
   "metadata": {},
   "source": [
    "## Purpose\n",
    " - To allow preprocessing + cross-validation\n",
    " - To avoid training info leaking into cross-validation set (via the `X_train_scaled`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda62db-9c1a-403c-b92c-289c4510e028",
   "metadata": {},
   "source": [
    "## How?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c153bc-59ce-412e-b0da-792f65100109",
   "metadata": {},
   "source": [
    "#### FIXME: Picture for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b240628-1723-4ff2-989c-0017aafec3c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| code-summary: optional\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#| code-fold: true\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# option 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      7\u001b[0m pip \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m      8\u001b[0m     steps \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 9\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mSimpleImputer\u001b[49m(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)), \u001b[38;5;66;03m# ← the earlier steps should be transformers\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler()),\n\u001b[1;32m     11\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m\"\u001b[39m, kNeighborsRegressor()),          \u001b[38;5;66;03m# ← the last step has to be model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleImputer' is not defined"
     ]
    }
   ],
   "source": [
    "#| code-summary: optional\n",
    "#| code-fold: true\n",
    "\n",
    "# option 1\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pip = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")), # ← the earlier steps should be transformers\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", kNeighborsRegressor()),          # ← the last step has to be model\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893d218-842f-438f-9a6b-db3ff118adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: Shorthand\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), \n",
    "    StandardScaler(), \n",
    "    kNeighborsRegressor()\n",
    ")\n",
    "\n",
    "pipe\n",
    "# The names are automatically defined from the lower case of the functions\n",
    "# e.g. SimpleImputer → \"simpleimputer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1a733-0bf7-4e35-ac0e-bd9ba5a9b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training only\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.predict(X_train)\n",
    "\n",
    "# Cross-validation\n",
    "cross_validate(pipe, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bd89d-5071-445a-82a2-89e617ee528d",
   "metadata": {},
   "source": [
    "## Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032d93f-cf55-4910-aa12-1a0db8a2e110",
   "metadata": {},
   "source": [
    " - All features are forced to go through the same transformations\n",
    "   - We want to apply OHE on categorical features, but NOT numeric features\n",
    "   - We want to apply scaling on numeric features, but NOT categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9722ab1-a3d9-4dd5-b7dc-fe8f78bfd846",
   "metadata": {},
   "source": [
    "# Combining ALL using **Column Transformer + Pipeline** (will mostly be used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3460d-8d87-4bd9-975c-35e73efdff13",
   "metadata": {},
   "source": [
    "## Purpose\n",
    " - In general, we want to apply different preprocessing/transformations on different features\n",
    "   - For numeric features: Imputation + Scaling\n",
    "   - For categorical features: Imputation + One-hot encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212a7dab-e32c-4802-be3e-fbbb2242f4ec",
   "metadata": {},
   "source": [
    "## How?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9080dc-5f7b-4bb0-85a4-187e87b835d8",
   "metadata": {},
   "source": [
    "### FIXME: Picture for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651e3a9-cc62-49b3-af0a-6cc403232c12",
   "metadata": {},
   "source": [
    "### 1) identifying feature type in the dataset, for example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d9280-ba0f-445c-a380-92e80e2e6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [<colname>, …]\n",
    "categorical_feats = [<colname>, …]\n",
    "passthrough_feats = [<colname>, …]\n",
    "drop_feats  = [<colname>, …] # for simpsity and demostration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa6e1b-6a95-4fb5-993f-ac813eafc4ac",
   "metadata": {},
   "source": [
    "### 2) apply on appropriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c53b7-477f-4eeb-90b5-a393aceb0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: optional\n",
    "#| code-fold: true\n",
    "\n",
    "# option 1\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "  [\n",
    "    (“scaling”, StandardScaler(), numeric_feats),\n",
    "    (“onehot”, OneHotEncoder(sparse=False), categorical_feats)\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add34df8-4a21-418a-8452-5c69130a3a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: Shorthand\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "ct = make_column_transformer(\n",
    "  (StandardScaler(), numeric_feats),\n",
    "  (OneHotEncoder(sparse=False), categorical_feats)\n",
    "  (“passthrough”, passthrough_feats),\n",
    "  (“drop”, drop_feats), ← the columns will be dropped even if we don’t have this line\n",
    ")\n",
    "ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d6055-cc69-4889-8da1-a4c27e9eb98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tran_array = ct.fit_transform(X_train) # return a np.ndarray\n",
    "\n",
    "column_names = (\n",
    "  numeric_feats\n",
    "  + ct.named_transformers_[“onehotencoder”].get_feature_names().tolist()\n",
    "  + passthrough_feats\n",
    ")\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ff1f7-0f89-4a88-a551-9d51cc4d00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tran = pd.DataFrame(X_train_tran_array, columns=column_names)\n",
    "X_train_tran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89851bea-3904-4f84-b8b4-fdde0cc4e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(ct, SVC())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec93c8-1b2b-464d-84a6-192217d37663",
   "metadata": {},
   "source": [
    "## Pros\n",
    " - Build all our transformations together into one object\n",
    "   - e.g. we would not forget to apply certain transformation in the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae50f2-efb0-44cc-9371-400948ce9d33",
   "metadata": {},
   "source": [
    "## Cons:\n",
    " - Problem with cross_validate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e0a54-f561-4be5-8f4d-9245c3ab4732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
